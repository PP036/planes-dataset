# Aircraft Detection System for Satellite Imagery

## 1. Project Overview
This project is a system for detecting aircraft in satellite imagery using computer vision and deep learning. It includes the complete data workflow: from dataset management to model training on a distributed cluster and monitoring results.

## 2. Repository Structure
- **`kaggle_data/`** - Original dataset from Kaggle platform
- **`dataset/`** - Prepared dataset for model training
    - `train/` - Training data (80%)
    - `test/` - Testing data (20%)
    - `data.yaml` - YOLO configuration file
- **`k8s/`** - Kubernetes configuration files
- **`docker/`** - Docker-related resources
- **`yolo-cpu/`** - YOLO scripts for CPU-based inference

## 3. Local Environment Setup

### 3.1 Label Studio Setup
```bash
# Launch Label Studio and MinIO containers
docker compose up -d
```

### 3.2 MinIO Configuration
- **URL:** http://localhost:9009/
- **Credentials:** minioadmin / minioadmin
- **Required Buckets:**
    - `planes-dataset` - Source images
    - `planes-labeled-dataset` - Labeled data from Label Studio
    - `planes-dvc-storage` - DVC data versioning storage

### 3.3 Label Studio Configuration
- **URL:** http://localhost:8080/
- **Setup Steps:**
    1. Register an account
    2. Create a project (Object Detection with Bounding Boxes)
    3. Configure cloud storage:
        - URL: http://minio:9000
        - Enable "Treat every bucket object as a source file"
        - Disable pre-signed URLs
    4. Upload images and perform annotations

## 4. Data Management Workflow

### 4.1 Data Annotation
1. Annotate images in Label Studio
2. Export to YOLO format using the utility script:
```bash
./export_yolo.sh
```

### 4.2 Data Versioning with DVC
Our dataset repository: https://github.com/PP036/planes-dataset

#### Initial Setup
```bash
# Install DVC and initialize repository
pip install 'dvc[all]'
dvc init
mkdir -p dataset

# Configure MinIO as remote storage
dvc remote add -d storage s3://cars-dvc-storage
dvc remote modify storage endpointurl http://localhost:9000

# Set MinIO credentials
export MINIO_ACCESS_KEY="minioadmin"
export MINIO_SECRET_KEY="minioadmin"
dvc remote modify storage --local access_key_id ${MINIO_ACCESS_KEY}
dvc remote modify storage --local secret_access_key ${MINIO_SECRET_KEY}

# Initial commit
git add .dvc .gitignore
git commit -m "Initialize DVC and configure storage"
```

#### Version Management Commands
```bash
# Track data changes
dvc add dataset
git add dataset.dvc .gitignore
git commit -m "Data version X"
dvc push

# Check data status and differences
dvc status
dvc diff

# Restore previous versions
git checkout <commit-hash>
dvc checkout

# Retrieve data in a fresh clone
export MINIO_ACCESS_KEY="minioadmin"
export MINIO_SECRET_KEY="minioadmin"
dvc remote modify storage --local access_key_id ${MINIO_ACCESS_KEY}
dvc remote modify storage --local secret_access_key ${MINIO_SECRET_KEY}
dvc pull
```

## 5. Distributed Training

### 5.1 Ray Cluster Setup
```bash
# Start head node
ray start --head --port=6379 --dashboard-host=0.0.0.0

# Connect worker nodes (run on each worker)
ray start --address='<head-node-IP-address>:6379'
```
- Ray Dashboard: http://localhost:8265

### 5.2 Dataset Distribution
```bash
# Prepare dataset archive
tar -czvf dataset.tar.gz dataset/

# Copy to all nodes and extract
for NODE in node1 node2 node3; do
    scp dataset.tar.gz user@${NODE}:/path/to/working/directory/
    ssh user@${NODE} "cd /path/to/working/directory/ && tar -xzvf dataset.tar.gz"
done
```

### 5.3 Model Training
```bash
# Start distributed training
python train_yolo.py --data dataset/data.yaml --epochs 100 --batch-size 16 --distributed
```

### 5.4 Training Monitoring with W&B
```bash
# Setup W&B
pip install wandb
wandb login
```
- Dashboard access: https://wandb.ai/username/planes-detection
- Tracked metrics: loss, precision, recall, mAP
- Visualizations: prediction results, experiment configuration, model artifacts

# Setting up a Local Kubernetes Cluster

## Command Line Tools Installation
```shell script
# Install kubectl
brew install kubectl

# Install kind for local Kubernetes cluster
brew install kind

# Install kustomize for YAML configuration management
brew install kustomize

# Install helm for Kubernetes package management
brew install helm
```

## Docker Desktop Installation
Install Docker Desktop with at least 8GB of allocated memory.

## Recommended Tools
For convenient cluster interaction, you can install a visual tool: https://k8slens.dev/

## Deploying Local K8s and Ray Cluster

### Preparation
The week-3/k8s/ directory already contains all necessary configuration files:
- kind/kind-config.yaml - local Kubernetes cluster configuration
- ray-cluster-values.yaml - Ray cluster parameters for Helm
- setup_cluster.sh - automatic deployment script

### Cluster Deployment
```shell script
# Navigate to the configuration directory
cd k8s/

# Make the script executable
chmod +x setup_cluster.sh

# Run automatic deployment
./setup_cluster.sh
```

The script will:
- Create a Kind cluster
- Install KubeRay operator via Helm
- Deploy Ray cluster with configured parameters
- Set up port-forwarding for service access

### Verifying Cluster Operation
```shell script
# Check pod status
kubectl get pods

# Check Ray cluster status
kubectl exec $(kubectl get pod -l ray.io/node-type=head -o jsonpath='{.items[0].metadata.name}') -- ray status

# Open Ray Dashboard in browser
http://localhost:8265
```

### Available Services
After successful deployment, the following will be available:
- Ray Dashboard: http://localhost:8265 - web interface for cluster monitoring
- Ray Client: ray://localhost:10001 - endpoint for Python client connection
- Ray Serve: http://localhost:8000 - endpoint for model deployment

### Running a Test Task
```shell script
# Via Ray Jobs API
ray job submit --address http://localhost:8265 -- python -c "import ray; ray.init(); print(ray.cluster_resources())"

# Or via Python client
python -c "
import ray
ray.init('ray://localhost:10001')
print('Ray cluster resources:', ray.cluster_resources())
ray.shutdown()
"
```

### Training a Model on KubeRay Using CPU
```shell script
cd yolo-cpu
python submit_job.py
```

### Cluster Management
```shell script
# Stop port forwarding
pkill -f 'kubectl port-forward.*raycluster-kuberay-head-svc'

# Delete Ray cluster
helm uninstall raycluster
helm uninstall kuberay-operator

# Completely delete Kind cluster
kind delete cluster --name ray-cluster

# Restart cluster from scratch
kind delete cluster --name ray-cluster
./setup_cluster.sh
```

## 6. –°–µ—Ä–≤—ñ–Ω–≥ –º–æ–¥–µ–ª–µ–π —É Ray
### 6.1 –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—è
- **–û—Ñ—ñ—Ü—ñ–π–Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—è:** [https://docs.ray.io/en/latest/serve/configure-serve-deployment.html](https://docs.ray.io/en/latest/serve/configure-serve-deployment.html)

### 6.2 –õ–æ–∫–∞–ª—å–Ω–µ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è
``` bash
# –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤—ñ–Ω–≥—É –º–æ–¥–µ–ª—ñ –ª–æ–∫–∞–ª—å–Ω–æ
serve run object_detection:entrypoint
```
### 6.3 –î–µ–ø–ª–æ–π –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä
``` bash
# –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤—ñ–Ω–≥—É –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä—ñ
python run_serve.py

# –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –ø–µ—Ä–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—è –ø–æ—Ä—Ç—ñ–≤
ps aux | grep "kubectl port-forward"
```
### 6.4 –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è API
``` bash
# –í—ñ–¥–ø—Ä–∞–≤–ª–µ–Ω–Ω—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –∑–∞–ø–∏—Ç—É –¥–æ API
python test.py
```
## CI/CD –¥–ª—è –¥–æ—Å—Ç–∞–≤–∫–∏ –º–æ–¥–µ–ª—ñ

–£ —Ü—å–æ–º—É –ø—Ä–æ—î–∫—Ç—ñ —Ä–µ–∞–ª—ñ–∑–æ–≤–∞–Ω–æ –ø–æ–≤–Ω–∏–π —Ü–∏–∫–ª CI/CD –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º **GitHub Actions**.

### üì¶ –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ (CI)

- –ó–∞–ø—É—Å–∫–∞—î—Ç—å—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –ø—Ä–∏ `pull_request` –≤ `main` –∞–±–æ –≤—Ä—É—á–Ω—É.
- –í—ñ–¥–ø—Ä–∞–≤–ª—è—î –∑–∞–¥–∞—á—É –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä Ray –¥–ª—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ.
- –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –ª–æ–≥—É—é—Ç—å—Å—è –≤ [Weights & Biases](https://wandb.ai/).

### üöÄ –î–µ–ø–ª–æ–π –º–æ–¥–µ–ª—ñ (CD)

- –ó–∞–ø—É—Å–∫–∞—î—Ç—å—Å—è –≤—Ä—É—á–Ω—É (`workflow_dispatch`).
- –í–∏–∫–æ–Ω—É—î –¥–µ–ø–ª–æ–π –Ω–æ–≤–æ—ó –≤–µ—Ä—Å—ñ—ó –º–æ–¥–µ–ª—ñ –Ω–∞ **Ray Serve**, —è–∫–∏–π –¥–æ—Å—Ç—É–ø–Ω–∏–π —á–µ—Ä–µ–∑ —ñ–Ω—Ç–µ—Ä—Ñ–µ–π—Å HTTP.
- –î–ª—è –¥–µ–ø–ª–æ—é –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è —Å–∫—Ä–∏–ø—Ç `week-4/ray-deploy/run_serve.py`.

### ‚öôÔ∏è –Ü–Ω—Å—Ç—Ä—É–∫—Ü—ñ—è –ø–æ –∑–∞–ø—É—Å–∫—É

1. **–¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è**
   - GitHub Actions ‚Üí Workflows ‚Üí `Train on Ray Cluster` ‚Üí `Run workflow`
2. **–î–µ–ø–ª–æ–π**
   - GitHub Actions ‚Üí Workflows ‚Üí `Deploy YOLOv8` ‚Üí `Run workflow`

> –ü–µ—Ä–µ–¥ –¥–µ–ø–ª–æ—î–º –ø–µ—Ä–µ–∫–æ–Ω–∞–π—Ç–µ—Å—å, —â–æ –∫–ª–∞—Å—Ç–µ—Ä Ray –∞–∫—Ç–∏–≤–Ω–∏–π —ñ `run_serve.py` –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –æ—Å—Ç–∞–Ω–Ω—é –º–æ–¥–µ–ª—å –∑ —Ä–µ—î—Å—Ç—Ä—É.

### üîê –°–µ–∫—Ä–µ—Ç–∏ GitHub (–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å—Å—è —É workflow)

- `WANDB_API_KEY`
- `WANDB_ENTITY`
- `WANDB_PROJECT

